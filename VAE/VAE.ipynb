{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numba import jit\n",
    "import time\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import scoreatpercentile \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "sys.path.append('./source')\n",
    "import toolkit\n",
    "sys.path.append('../')\n",
    "\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The -n argument of ./preprocessing.py\n",
    "proteinname = 'SH3'\n",
    "path = 'Outputs/'\n",
    "\n",
    "# Import the Potts sequence. \n",
    "parameters = pickle.load(open(path + proteinname + \".db\", 'rb'))\n",
    "index = parameters['index']\n",
    "q_n = parameters['q_n'] # Number of possible residues on each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seq('DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET', SingleLetterAlphabet()), Seq('DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET', SingleLetterAlphabet()), Seq('DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET', SingleLetterAlphabet()), Seq('DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET', SingleLetterAlphabet()), Seq('DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET', SingleLetterAlphabet()), Seq('DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET', SingleLetterAlphabet()), Seq('-MNHRLTALYNF-DEDDRQMSVFRGDVVYVLEQ-EHWWFVCRSRKEGWVPAWFLCYFNN', SingleLetterAlphabet()), Seq('AGSSIAVALFEFNSDSPDGSSLKKSDLVTVLAEPVSWWQLEPNRREGLVLVTYLELISI', SingleLetterAlphabet()), Seq('DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET', SingleLetterAlphabet()), Seq('DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET', SingleLetterAlphabet()), Seq('DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET', SingleLetterAlphabet())]\n"
     ]
    }
   ],
   "source": [
    "alignment = 'test.fasta'\n",
    "sequence = [str(i) for i in toolkit.get_seq(alignment)]\n",
    "sequence = [i[:16]+i[18:44]+i[45:] for i in toolkit.get_seq(alignment)]\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa = 'Inputs/sh3_59.fasta'\n",
    "msa = [str(i) for i in toolkit.get_seq(msa)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET\n",
      "11\n",
      "DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET\n",
      "11\n",
      "DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET\n",
      "11\n",
      "DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET\n",
      "11\n",
      "DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET\n",
      "11\n",
      "DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET\n",
      "11\n",
      "-MNHRLTALYNF-DEDDRQMSVFRGDVVYVLEQ-EHWWFVCRSRKEGWVPAWFLCYFNN\n",
      "11\n",
      "AGSSIAVALFEFNSDSPDGSSLKKSDLVTVLAEPVSWWQLEPNRREGLVLVTYLELISI\n",
      "11\n",
      "DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET\n",
      "11\n",
      "DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET\n",
      "11\n",
      "DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "v_traj_onehot, _ = toolkit.convert_potts(sequence, index)\n",
    "print(len(v_traj_onehot[:,1]))\n",
    "\n",
    "for i in range(len(sequence)):\n",
    "    print(sequence[i])\n",
    "    print(len(sequence))\n",
    "#v_traj_onehot, _ = toolkit.convert_potts(msa, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Z from arbitray made up sequence AAAAAAAALYDFQSDKENELDIKAGEIIQIVSKENGWWLCKNTYAQGWTPEAYVEEQVA\n",
    "al = 'test.fasta'\n",
    "seqs = [str(i) for i in toolkit.get_seq(al)]\n",
    "seqs = [i[:16]+i[18:44]+i[45:] for i in toolkit.get_seq(al)]\n",
    "v_traj_onehot, q_n = toolkit.convert_potts(seqs, index)\n",
    "\n",
    "\n",
    "N=np.size(v_traj_onehot,axis=0) #number of samples \n",
    "q=np.size(v_traj_onehot,axis=1) #number of one-hot features\n",
    "n=np.size(q_n) # number of amino acid residues in a sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, q=q, d=3):\n",
    "        super(VAE, self).__init__()\n",
    "        self.hsize=int(1.5*q) # size of hidden layer\n",
    "        \n",
    "        self.en1 = nn.Linear(q, self.hsize)\n",
    "        self.en2 = nn.Linear(self.hsize, self.hsize) #\n",
    "        self.en3 = nn.Linear(self.hsize, self.hsize)\n",
    "        self.en_mu = nn.Linear(self.hsize, d)\n",
    "        self.en_std = nn.Linear(self.hsize, d) # Is it logvar?\n",
    "        \n",
    "        self.de1 = nn.Linear(d, self.hsize)\n",
    "        self.de2 = nn.Linear(self.hsize, self.hsize) #\n",
    "        self.de22 = nn.Linear(self.hsize, self.hsize)\n",
    "        self.de3 = nn.Linear(self.hsize, q)     \n",
    " \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(self.hsize) # batchnorm layer\n",
    "        self.bn2 = nn.BatchNorm1d(self.hsize)\n",
    "        self.bn3 = nn.BatchNorm1d(self.hsize)\n",
    "        self.bnfinal = nn.BatchNorm1d(q)  \n",
    "\n",
    "        #replace tanh with relu\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode a batch of samples, and return posterior parameters for each point.\"\"\"\n",
    "        x = self.tanh(self.en1(x)) # first encode\n",
    "        x = self.dropout1(x) \n",
    "        x = self.tanh(self.en2(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.tanh(self.en3(x)) # second encode\n",
    "        return self.en_mu(x), self.en_std(x) # third (final) encode, return mean and variance\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode a batch of latent variables\"\"\"\n",
    "        z = self.tanh(self.de1(z))\n",
    "        z = self.bn2(z)\n",
    "        z = self.tanh(self.de2(z))\n",
    "        z = self.dropout2(z)\n",
    "        z = self.tanh(self.de22(z))\n",
    "        \n",
    "        # residue-based softmax\n",
    "        # - activations for each residue in each position ARE constrained 0-1 and ARE normalized (i.e., sum_q p_q = 1)\n",
    "        z = self.bn3(z)\n",
    "        z = self.de3(z)\n",
    "        z = self.bnfinal(z)\n",
    "        z_normed = torch.FloatTensor() # empty tensor?\n",
    "        z_normed = z_normed.to(device) # store this tensor in GPU/CPU\n",
    "        for j in range(n):\n",
    "            start = np.sum(q_n[:j])\n",
    "            end = np.sum(q_n[:j+1])\n",
    "            z_normed_j = self.softmax(z[:,start:end])\n",
    "            z_normed = torch.cat((z_normed,z_normed_j),1)\n",
    "        return z_normed\n",
    "    \n",
    "    def reparam(self, mu, logvar): \n",
    "        \"\"\"Reparameterisation trick to sample z values. \n",
    "        This is stochastic during training, and returns the mode during evaluation.\n",
    "        Reparameterisation solves the problem of random sampling is not continuous, which is necessary for gradient descent\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_() \n",
    "            eps = std.data.new(std.size()).normal_() # normal distribution\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu      \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Takes a batch of samples, encodes them, and then decodes them again to compare.\"\"\"\n",
    "        mu, logvar = self.encode(x.view(-1, q)) # get mean and variance\n",
    "        z = self.reparam(mu, logvar) # sampling latent variable z from mu and logvar\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    def loss(self, reconstruction, x, mu, logvar): \n",
    "        \"\"\"ELBO assuming entries of x are binary variables, with closed form KLD.\"\"\"\n",
    "        bce = torch.nn.functional.binary_cross_entropy(reconstruction, x.view(-1, q))\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        # Normalise by same number of elements as in reconstruction\n",
    "        KLD /= x.view(-1, q).data.shape[0] * q \n",
    "        return bce + KLD\n",
    "    \n",
    "    def get_z(self, x):\n",
    "        \"\"\"Encode a batch of data points, x, into their z representations.\"\"\"\n",
    "        print(x.type)\n",
    "        mu, logvar = self.encode(x.view(-1, q))\n",
    "        return self.reparam(mu, logvar)\n",
    "    \n",
    "    def sequence_from_z(self, z):\n",
    "        parameters = pickle.load(open(\"Outputs/SH3.db\", 'rb'))\n",
    "        q_n = parameters['q_n']\n",
    "        index = parameters['index']\n",
    "        v_traj_onehot = parameters['onehot']\n",
    "        \n",
    "        N=np.size(v_traj_onehot,axis=0)\n",
    "        q=np.size(v_traj_onehot,axis=1)\n",
    "        n=np.size(q_n)\n",
    "        z_gen=z\n",
    "        \n",
    "        data = torch.FloatTensor(z_gen).to(device)\n",
    "        data = self.decode(data)\n",
    "        \n",
    "        v_gen = data.cpu().detach().numpy()\n",
    "        sample_list = []\n",
    "\n",
    "        for i in range(len(z_gen)): # number of sampling points\n",
    "            v_samp_nothot = toolkit.sample_seq(0, q, n, q_n, i, v_gen)\n",
    "            sample_list.append(v_samp_nothot)\n",
    "\n",
    "        sequences = toolkit.convert_alphabet(np.array(sample_list), index, q_n)\n",
    "        return sequences\n",
    "    def z_from_sequence(self, sequence):\n",
    "        seq = sequence\n",
    "        v_traj_onehot, q_n = toolkit.convert_potts(seq, index)\n",
    "        print(q_n)\n",
    "        z = self.get_z(torch.FloatTensor(v_traj_onehot)).cpu().detach().numpy()\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DSAARAKVLFDFARGNENELPIKAGEIIQIVSKENGWWLTKNTAKAGWTPAAYVEKEET']\n",
      "<built-in method type of Tensor object at 0x7ff2d2d73b90>\n",
      "[[0.35427824 0.32403612 0.22891119]\n",
      " [0.6953942  1.8044727  0.0173651 ]]\n"
     ]
    }
   ],
   "source": [
    "a = \"DSAARAKVLFDFARGNDDENELPIKAGEIIQIVSKENGWWLTKNATAKAGWTPAAYVEKEET\"\n",
    "a = [i[:16]+i[18:44]+i[45:] for i in [a]]\n",
    "print(a)\n",
    "\n",
    "v_traj_newortho, _ = toolkit.convert_potts([\"WFPYKAKALYSYQADDIYEISFTKGEILDVGDIGGRWWKARRNGEVGIIPSNYVQLDED\",\"WFPYKAKALYSYQADDIYEISFTKGEILDVGDIGGRWWKARRNGEVGIIPSNYVQLDED\"],index)\n",
    "z_2 = model_2.get_z(torch.FloatTensor(v_traj_newortho)).cpu().detach().numpy()\n",
    "print(z_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-ddf86f61d270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mv_traj_newortho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoolkit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_potts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WFPYKAKALYSYQADDIYEISFTKGEILDVGDIGGRWWKARRNGEVGIIPSNYVQLDED'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_traj_newortho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "v_traj_newortho, _ = toolkit.convert_potts(['WFPYKAKALYSYQADDIYEISFTKGEILDVGDIGGRWWKARRNGEVGIIPSNYVQLDED']*2, index)\n",
    "z_2 = model.get_z(torch.FloatTensor(v_traj_newortho)).cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'z_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-69f96fe8e4ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'z_2' is not defined"
     ]
    }
   ],
   "source": [
    "print(z_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
